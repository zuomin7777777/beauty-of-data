<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Sharing the beauty of data">
  <meta name="generator" content="Hugo 0.19" />

  <title>Learning Notes &middot; Lithium Theme</title>

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="../../css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="../../css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="../../css/blackburn.css">

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  
  <link rel="alternate" type="application/rss+xml" title="Lithium Theme" href="../../tags/learning-notes/index.xml" />
  

  

  <link rel="shortcut icon" href="../../img/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  

  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="../../about/">About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://github.com/elara7">GitHub</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="http://weibo.com/elara">Weibo</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="../../tags/learning-notes/index.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small></small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Learning Notes</h1>
</div>

<div class="content">
  
    <article>
  <header>
    <h2><a href="../../2016/07/07/advanced-r-notes-1.data-structures/">Advanced R Notes 1.Data structures</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>07 Jul 2016, 00:00</time>
  </div>

  

  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="../../tags/learning-notes">Learning notes</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  Advanced R Notes 1.Data structuresElara
Thanks for Hadley Wickham. Source available on github (https://github.com/hadley/adv-r/).
分类：atomic vectors，lists通用属性： type，typeof（），内含的元素的type（如logical,integer,double(numeric),character等）。所有元素type都相同则为atomic vectors，不同为lists。
Length，length（），元素个数（向量长度）
Attributes，attributes（），额外的信息
区分： is.vector（）：当且仅当object是一个除了names以外，没有其他额外attributes的vector的时候，返回true is.atomic（）：测试是否是一个atomic vector is.list（）：测试是否是一个listAtomic vector：创建方法：c（）
c（c（c（）））嵌套后仍然会flat（链接成为一个向量）
内含元素的type：
common types：logical，integer（用1L,2L强制设定），double（numeric），character
Rare types：complex，raw
Types and tests：is.character(), is.double(), is.integer(), is.logical(), or, more generally, is.atomic().is.numeric() 在interger和double的时候都返回true
Coercion：Types from least to most flexible are: logical, integer, double, and character.2个不同type用c合并的时候会（按上述顺序）从前向后转化listslists是一个内含元素type不同的向量，内含元素可以是一个list
- 创建方法：list（）
  </p>

  
  <footer>
    <a href="../../2016/07/07/advanced-r-notes-1.data-structures/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="../../2016/06/29/note0-for-nlp/">Note0 for NLP</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>29 Jun 2016, 00:00</time>
  </div>

  

  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="../../tags/learning-notes">Learning notes</a>
    
  </div>
  
  

</div>

  </header>

  <p>
  自然语言处理入门笔记统计语言模型现代自然语言处理方法基于统计学而非规则设计。通过规则设计难以分析复杂句子并且无法快速对新生用语做出反应。句子出现概率：
一个句子S由一连串特定排列的词\(w_1,w_2,\dots,w_n\)构成，其中n为句子长度，那么根据条件概率公式，这个句子在文本中出现个概率为\[P(s)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2) \dots P(w_n|w_1,w_2,\dots)\]
显然，对于上述句子出现概率的算法太过复杂，因此引入一些假设进行简化。如引入马尔科夫假设，即假设每个词出现个概率都只和前一个词有关，与其他词无关，则句子出现概率可以简化为\[P(s)=P(w_1)P(w_2|w_1)P(w_3|w_2) \dots \],此即为所谓的二元模型（bigram model），同理若假设每个词出现的概率只和前面N个词有关，则可建立N元模型。模型元数越多越复杂，越难估计。
对于每个词出现的条件概率可化为\[P(w_i|w_j)=\frac{P(w_i,w_j)}{P(w_j)}\]，（这里如果是二元模型则j=i-1），其中每个词或2个词出现的概率可用其频率进行估计，即\[P(w_i,w_j) = \frac{count(w_j,w_i)}{count}\],\[P(w_j) = \frac{count(w_j)}{count}\]，所以\[P(w_i|w_j) = \frac{count(w_j,w_i)}{count(w_j)}\]
然而很多词及其组合出现频率较小，甚至在样本中无法出现，解决办法：古德-图灵估计。从概率的总量中分配一小部分给没有在样本中观察到的事件，同时对看得见的事件中，出现频率小的事件再调小其概率（即“越是不可信的统计折扣越多”）。具体来说，如果在语料库中出现r次的词有\(N_r\)个，对于出现频率较小的词（r较小），计算其概率时用更小的概率\(d_r=(r+1)\frac{N_R}{N_r}/count\),这里R=r+1,则没有出现的词有\(d_0=N_1/N_0\),得到一个很小的正数概率，从而解决这个问题。即当\[count(w_i) &lt;= T\],\[P(w_i) = \frac{count(w_i)}{count}\],否则\[P(w_i) = d_r=(r+1)\frac{N_R}{N_r}\]。同理对于\[P(w_i|w_j)\]也可以类似处理，经过古德-图灵方法打折求条件概率的方法叫卡茨退避法。其中T是一个阈值一般是8-10左右。
训练模型的语料库的选取，要根据模型应用的领域进行。训练模型的过程基本上和训练隐马儿可夫模型类似，需要一个训练算法（鲍姆-韦尔奇算法）和使用的解码算法（维特比算法）。分词算法思想：假设一个句子有N种分法，每种分法都可以根据上述方法求出这个句子出现的概率。那么最优的分法就是让这个句子出现概率最大的分法。（极大似然的思想）
实现方法：对每种分法都进行计算显然计算量太大，通常转换成一个动态规划问题，根据维特比算法找到最佳分词。算法已经得到较高的准确度，再深入研究进行准确度的提升已经不容易。同时不同的人之间分词都有差异，机器分词的准确率小幅度的差异并不能作为评判好坏的唯一标准。
分词的颗粒度：即对一个整体词的细分程度，如清华大学是否要再分为清华和大学。一个分词器一般都可以由用户自行决定颗粒度。完善用于再细分的复合词的词典（通过数据挖掘）是近年中文分词工作重点。信息论概念信息熵：H(X),事件X或者信息X内涵的信息量。H(X|Y)：条件熵，已知Y条件下H事件的信息量。更多：中文信息熵
互信息：两个随机事件相关性的度量方法（类似相关系数，但是相关系数要用于随机变量），定义为\[I(X;Y)=\sum P(x,y)log\frac{P(x,y)}{P(x)P(y)}=H(X)-H(X|Y)\]。应用：机器翻译的时候Bush可以翻译为布什和灌木丛，做法是先从大量文本中找出和布什一起出现的互信息最大的词，同理找出和灌木丛一起出现的互信息最大的词，则翻译的时候看上下文哪一类词多就可以了。
相对熵。关键词权重度量方法：TF-IDFTF:term frequency,单文本词频，即一组关键词的各个词，在文本中出现的频率之和（若只是一个词，则就是这个词的频率）。但是如果直接根据分词从一个句子中分出的词可能含有“的”这种没用的词，叫做停止词，应当被忽略。只用TF会使得关键词只能体现高频词，而无法体现一些特定场合才会出现的重要词汇。所以还要用一个权重IDF：inverse document frequency，逆文本频率指数，公式为\(log\frac{D}{D_w}\)，D是总文章数或者总网页数（语料库总数），\(D_w\)是语料库中含有某个词的文章数或网页数。则一个关键词w的TF-IDF值为\(TF_w \cdot IDF_w\)，一组词或者一句话的TF-IDF值为\(\sum TF_i \cdot IDF_i\),i对应这句话中的每个词（去掉停止词，或者说停止词权重设定为0）。TF-IDF度量的是一句话或者一个词在一篇文本或者一个网页中的重要度。
文本特征向量对一篇文本的所有实词（虚词对于文本的内容没有太大信息量，可以认为是噪音，剔除后可以提高效率），计算其TF-IDF值，然后按照词汇表中每个词的位置（如词汇表中第一个词是阿，第二个是啊，第700个是服装），把文章中出现的词TF-IDF对应放到对应位置，没有出现的词设定TF-IDF为0，那么就得到长度和词汇表词数相等的一个向量，这个向量就是这篇文章的特征向量。
若要度量文本的相似度，就可以直接计算不同文本的特征向量的余弦距离。从而达到文本聚类的目的，首先把文本两两计算余弦距离，最近的分为一类，划分成几个小类，把小类中的文本全部合并当成一篇，计算其特征向量作为类的特征向量，再次计算余弦距离，再次分类，以此类推直到相关性很弱，停止聚类。另外，出现在标题等重要位置的词的意义应该更大，可以调整权重使其权重更大。
奇异值分解（SVD：singular value decomposition）如果有N个词，M篇文本，如果把每篇文本的特征向量作为一行，则M篇文本可以合并成一个M行N列的矩阵。通过SVD可以得到3个矩阵X B Y，X是一个Mxn的矩阵，每一行表示一篇文本，每一列表示一个主题（假设分解出了n个主题），则每个元素表示这篇文本与这个主题的相关性。Y为一个nxN的矩阵，则每一列表示一个词，每一行表示一个词类，每个元素表示这个词与这个词类的相关性。B为一个nxn矩阵，每一行表示一个主题，每一列表示一个词类，每个元素则表示一个词类和一个主题的相关性。通过SVD就可以粗略地把一个语料库中的所有文本进行主题分类和词类分类（近义词分类）。这和前面的余弦距离聚类相比，优点是速度更快，不需要一次次迭代，缺点是精度不足且内存需求大。可以先进行SVD作为粗分类，再用余弦距离法继续迭代。
  </p>

  
  <footer>
    <a href="../../2016/06/29/note0-for-nlp/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
</div>

</div>
</div>
<script src="../../js/ui.js"></script>




</body>
</html>
